# -*- coding: utf-8 -*-
"""Text_Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-5j-e8ONqCL6rixYpPLHJWGx9iJtQHC
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import networkx as nx

nltk.download('punkt')
nltk.download('stopwords')

# Sample text for summarization
text = """
Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.
Colloquially, the term "artificial intelligence" is often used to describe machines (or computers) that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving".
As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. """

# Tokenize the text into sentences
sentences = sent_tokenize(text)

# Tokenize the sentences into words
words = [word_tokenize(sentence.lower()) for sentence in sentences]

# Remove stopwords
stop_words = set(stopwords.words("english"))
filtered_words = [[word for word in sentence if word not in stop_words] for sentence in words]

# Calculate word frequencies
word_frequencies = FreqDist([word for sentence in filtered_words for word in sentence])

# Create a graph representation of the sentences
graph = nx.Graph()
for i, sentence in enumerate(filtered_words):
    for word in sentence:
        graph.add_node(word)
    for j in range(i+1, len(filtered_words)):
        common_words = set(sentence) & set(filtered_words[j])
        weight = sum([1/word_frequencies[word] for word in common_words])
        if weight > 0:
            graph.add_edge(i, j, weight=weight)

# Apply TextRank algorithm for sentence ranking
scores = nx.pagerank(graph)

# Sort sentences by their scores
ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)

N = 2
summary_sentences = sorted(ranked_sentences[:N], key=lambda x: x[0])

# Construct the summary
summary = " ".join(sentence for _, sentence in summary_sentences)
print(summary)

